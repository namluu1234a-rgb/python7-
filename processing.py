import pandas as pd
import numpy as np
import os
from scipy.stats import pearsonr
from wordcloud import WordCloud
import matplotlib.pyplot as plt

plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
DATA_DIR = "./real_data"

# è‚¡ç¥¨æ¸…å•
STOCKS = {
    '002242': {'name': 'ä¹é˜³è‚¡ä»½', 'type': 'noise'},
    '601127': {'name': 'èµ›åŠ›æ–¯', 'type': 'value'},
    '01810': {'name': 'å°ç±³é›†å›¢', 'type': 'value'}
}


def process_final():
    print("ğŸš€ å¯åŠ¨è·¨å¹³å°èˆ†æƒ…èåˆå¼•æ“ (Guba + Bilibili)...")
    stats_list = []

    for code, info in STOCKS.items():
        name = info['name']

        # 1. å®šä¹‰æ–‡ä»¶è·¯å¾„
        guba_path = f"{DATA_DIR}/sentiment_{code}.csv"
        bili_path = f"{DATA_DIR}/bilibili_{code}.csv"
        market_path = f"{DATA_DIR}/market_{code}.csv"

        # æ£€æŸ¥å¸‚åœºæ•°æ® (å¿…é¡»æœ‰)
        if not os.path.exists(market_path):
            print(f"âš ï¸ è·³è¿‡ {name}: ç¼ºè‚¡ä»·æ•°æ®")
            continue

        # 2. è¯»å–å„è·¯æ•°æ®
        df_m = pd.read_csv(market_path, index_col=0)
        df_m.index = pd.to_datetime(df_m.index, errors='coerce')

        # è¯»å–è‚¡å§
        if os.path.exists(guba_path):
            df_guba = pd.read_csv(guba_path, index_col=0)
            df_guba.index = pd.to_datetime(df_guba.index, errors='coerce')
            df_guba = df_guba.rename(columns={'read_count': 'guba_buzz'})
            # ç¡®ä¿åˆ—å­˜åœ¨
            if 'guba_buzz' in df_guba.columns:
                df_guba = df_guba[['guba_buzz']]
            else:
                df_guba['guba_buzz'] = 0
        else:
            df_guba = pd.DataFrame(columns=['guba_buzz'])

        # è¯»å–Bç«™
        if os.path.exists(bili_path):
            df_bili = pd.read_csv(bili_path, index_col=0)
            df_bili.index = pd.to_datetime(df_bili.index, errors='coerce')
            if 'bili_buzz' in df_bili.columns:
                df_bili = df_bili[['bili_buzz']]
            else:
                df_bili['bili_buzz'] = 0
        else:
            df_bili = pd.DataFrame(columns=['bili_buzz'])

        # 3. è·¨å¹³å°æ•°æ®èåˆ (Outer Join)
        # è¿™ä¸€æ­¥æŠŠè‚¡å§å’ŒBç«™çš„æ—¶é—´è½´å¹¶é›†ï¼Œå“ªå¤©æ²¡æ•°æ®å°±å¡«0
        df_social = pd.merge(df_guba, df_bili, left_index=True, right_index=True, how='outer')
        df_social = df_social.fillna(0)

        # 4. è®¡ç®—å…¨ç½‘æ€»çƒ­åº¦
        if 'guba_buzz' not in df_social.columns: df_social['guba_buzz'] = 0
        if 'bili_buzz' not in df_social.columns: df_social['bili_buzz'] = 0

        df_social['total_buzz'] = df_social['guba_buzz'] + df_social['bili_buzz']

        print(f"\nğŸ”¨ å¤„ç† {name}: è‚¡å§+Bç«™ -> èåˆå{len(df_social)}å¤©")

        # 5. äº¤æ˜“æ—¥å¯¹é½ä¸é€’å»¶ (Weekend Effect)
        trade_days = df_m.index.sort_values()

        def get_next_trade_day(d):
            future_days = trade_days[trade_days >= d]
            return future_days[0] if len(future_days) > 0 else pd.NaT

        df_social['trade_date'] = df_social.index.to_series().apply(get_next_trade_day)
        df_social = df_social.dropna(subset=['trade_date'])

        # æŒ‰äº¤æ˜“æ—¥èšåˆ
        df_social_agg = df_social.groupby('trade_date').agg({
            'total_buzz': 'sum',
            'guba_buzz': 'sum',
            'bili_buzz': 'sum'
        })

        # 6. ä¸è‚¡ä»·åˆå¹¶ ã€æ ¸å¿ƒä¿®å¤ç‚¹ã€‘
        # è¿™é‡Œä¹‹å‰å†™é”™äº†å˜é‡åï¼Œç°åœ¨ä¿®æ­£ä¸º df_social_agg
        df_final = pd.merge(df_m, df_social_agg, left_index=True, right_index=True, how='left')

        df_final['total_buzz'] = df_final['total_buzz'].fillna(0)

        # 7. è®¡ç®—ç´¯ç§¯è¶‹åŠ¿å› å­ (Cumulative Trend)
        df_final['cum_factor'] = df_final['total_buzz'].cumsum()

        # å½’ä¸€åŒ– (0-100)ï¼Œæ–¹ä¾¿ç”»å›¾å’ŒAPPå±•ç¤ºï¼Œå‘½åä¸º meme_heat
        # é¿å…é™¤ä»¥0
        denom = df_final['cum_factor'].max() - df_final['cum_factor'].min()
        if denom == 0: denom = 1

        df_final['meme_heat'] = (df_final['cum_factor'] - df_final['cum_factor'].min()) / denom

        # 8. ç»Ÿè®¡åˆ†æ
        valid_df = df_final.dropna(subset=['CAR', 'meme_heat'])

        if len(valid_df) > 5:
            corr, p = pearsonr(valid_df['meme_heat'], valid_df['CAR'])
            print(f"   ğŸ“Š èåˆåæ•ˆæœ: R={corr:.4f} (P={p:.4e})")

            # ä¿å­˜æœ€ç»ˆå®½è¡¨
            df_final.to_csv(f"{DATA_DIR}/final_{code}.csv")

            # è®°å½•ç»Ÿè®¡ç»“æœ
            total_buzz_sum = df_social['total_buzz'].sum() + 1
            stats_list.append({
                'code': code, 'name': name,
                'r': corr, 'p': p,
                'guba_ratio': df_social['guba_buzz'].sum() / total_buzz_sum,
                'bili_ratio': df_social['bili_buzz'].sum() / total_buzz_sum
            })

            # ç”Ÿæˆæ··åˆè¯äº‘ (å…œåº•)
            wc_path = f"{DATA_DIR}/wc_{code}.png"
            if not os.path.exists(wc_path):
                wc = WordCloud(font_path="C:/Windows/Fonts/simhei.ttf", background_color="white", width=800, height=500)
                wc.generate(name)
                wc.to_file(wc_path)
        else:
            print("   âš ï¸ æœ‰æ•ˆæ•°æ®ä¸è¶³ï¼Œæ— æ³•å›å½’")

    # ä¿å­˜ç»Ÿè®¡è¡¨
    if stats_list:
        stat_df = pd.DataFrame(stats_list)
        stat_df.to_csv(f"{DATA_DIR}/stats.csv", index=False)
        print("\nâœ… å…¨æµç¨‹ç»“æŸï¼ç»Ÿè®¡ç»“æœå·²ä¿å­˜ã€‚")


if __name__ == "__main__":
    process_final()
