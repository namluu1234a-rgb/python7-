import pandas as pd
import time
import random
import re
import os
import concurrent.futures
from selenium import webdriver
from selenium.webdriver.edge.service import Service as EdgeService
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.microsoft import EdgeChromiumDriverManager
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.webdriver.edge.options import Options as EdgeOptions
from bs4 import BeautifulSoup

# ==========================================
# 1. æ ¸å¿ƒé…ç½® (å·²å¡«å…¥ä½ æä¾›çš„ç²¾å‡†åæ ‡)
# ==========================================
CONFIG = {
    # 1. ä¹é˜³è‚¡ä»½ (2025å¹´11æœˆ - å“ˆåŸºç±³è¥é”€)
    '002242': {
        'name': 'ä¹é˜³è‚¡ä»½',
        'start_page': 1, 'end_page': 51,
        'target_year': '2025',
        'url_code': '002242'
    },
    # 2. èµ›åŠ›æ–¯ (2023å¹´9-11æœˆ - é¥é¥é¢†å…ˆ)
    '601127': {
        'name': 'èµ›åŠ›æ–¯',
        'start_page': 2940, 'end_page': 3659,
        'target_year': '2023',
        'url_code': '601127'
    },
    # 3. å°ç±³é›†å›¢ (2024å¹´3-4æœˆ - é›·ç¥SU7)
    '01810': {
        'name': 'å°ç±³é›†å›¢',
        'start_page': 1953, 'end_page': 2069,
        'target_year': '2024',
        'url_code': 'hk01810'
    }
}

RAW_DATA_DIR = "./raw_data_lake"
if not os.path.exists(RAW_DATA_DIR): os.makedirs(RAW_DATA_DIR)

# å¼€å¯ 10 çº¿ç¨‹åŠ é€Ÿ (èµ›åŠ›æ–¯æœ‰700å¤šé¡µï¼Œå¿…é¡»å¤šçº¿ç¨‹)
MAX_WORKERS = 10


# ==========================================
# 2. æé€Ÿæµè§ˆå™¨é©±åŠ¨ (æ— å¤´+ç¦å›¾)
# ==========================================
def get_fast_driver():
    try:
        options = EdgeOptions()
        options.add_argument('--headless')  # åå°è¿è¡Œ
        options.add_argument('--disable-gpu')
        # ç¦æ­¢å›¾ç‰‡åŠ è½½ï¼Œæå¤§æå‡é€Ÿåº¦
        prefs = {"profile.managed_default_content_settings.images": 2}
        options.add_experimental_option("prefs", prefs)
        options.page_load_strategy = 'eager'
        options.add_experimental_option('excludeSwitches', ['enable-logging'])
        service = EdgeService(EdgeChromiumDriverManager().install())
        return webdriver.Edge(service=service, options=options)
    except:
        try:
            # å¤‡é€‰ Chrome
            options = ChromeOptions()
            options.add_argument('--headless')
            options.add_argument('--disable-gpu')
            prefs = {"profile.managed_default_content_settings.images": 2}
            options.add_experimental_option("prefs", prefs)
            options.page_load_strategy = 'eager'
            options.add_experimental_option('excludeSwitches', ['enable-logging'])
            service = ChromeService(ChromeDriverManager().install())
            return webdriver.Chrome(service=service, options=options)
        except:
            return None


# ==========================================
# 3. æŠ“å–é€»è¾‘ (åªæ¬è¿ï¼Œä¸è®¡ç®—)
# ==========================================
def worker_crawl(url_code, start_page, end_page, year, worker_id):
    driver = get_fast_driver()
    if not driver: return []

    local_data = []
    # æ­£åˆ™ï¼šåªåŒ¹é… MM-DD
    date_pattern = re.compile(r'(0[1-9]|1[0-2])-(0[1-9]|[12]\d|3[01])')

    try:
        for page in range(start_page, end_page + 1):
            url = f"http://guba.eastmoney.com/list,{url_code}_{page}.html"
            try:
                driver.get(url)
                time.sleep(random.uniform(0.3, 0.6))  # æé€Ÿç¿»é¡µ

                soup = BeautifulSoup(driver.page_source, 'html.parser')
                items = soup.select('.listitem')
                if not items: items = soup.find_all('tr')

                for item in items:
                    try:
                        text = item.text.strip()

                        # æå–æ ‡é¢˜
                        title_tag = item.select_one('.l3 a')
                        if not title_tag:
                            links = item.find_all('a')
                            title_tag = max(links, key=lambda x: len(x.text)) if links else None

                        title = title_tag.get('title') or title_tag.text if title_tag else ""
                        title = title.strip()
                        if not title or "å…¬å‘Š" in title or "èµ„è®¯" in title: continue

                        # æå–æ—¥æœŸå¹¶æ‹¼ä¸Šå¹´ä»½
                        match = date_pattern.search(text)
                        if match:
                            full_date = f"{year}-{match.group(0)}"

                            read_count = 0
                            nums = re.findall(r'\d+', text.replace('ä¸‡', '0000'))
                            if nums: read_count = int(nums[0])

                            local_data.append({
                                'date': full_date,
                                'title': title,
                                'read_count': read_count
                            })
                    except:
                        continue
            except:
                continue

            # è¿›åº¦æç¤º
            if (page - start_page) % 50 == 0:
                print(f"   âš¡ [çº¿ç¨‹-{worker_id}] {year}å¹´æ•°æ® | è¿›åº¦: {page}é¡µ")

    finally:
        driver.quit()

    return local_data


# ==========================================
# 4. ä¸»ç¨‹åº
# ==========================================
def run_scraper():
    total_start = time.time()

    for code, conf in CONFIG.items():
        name = conf['name']
        start, end, year = conf['start_page'], conf['end_page'], conf['target_year']
        u_code = conf['url_code']

        print(f"\n==============================================")
        print(f"ğŸš€ å¯åŠ¨æ”¶å‰²æœº: {name} ({year}å¹´)")
        print(f"ğŸ“„ ä»»åŠ¡èŒƒå›´: {start} - {end} é¡µ")
        print(f"==============================================")

        chunk_size = (end - start + 1) // MAX_WORKERS + 1
        futures = []
        all_results = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            for i in range(MAX_WORKERS):
                c_start = start + i * chunk_size
                c_end = min(start + (i + 1) * chunk_size - 1, end)
                if c_start > end: break
                futures.append(executor.submit(worker_crawl, u_code, c_start, c_end, year, i + 1))

            for future in concurrent.futures.as_completed(futures):
                all_results.extend(future.result())

        if all_results:
            df = pd.DataFrame(all_results)
            save_path = f"{RAW_DATA_DIR}/raw_{code}.csv"
            df.to_csv(save_path, index=False, encoding='utf-8-sig')
            print(f"âœ… {name} æŠ“å–å®Œæ¯•ï¼å­˜å…¥: {save_path} (å…± {len(df)} æ¡)")
        else:
            print(f"âš ï¸ {name} æœªæŠ“åˆ°æ•°æ®")

    print(f"\nğŸ å…¨éƒ¨è€—æ—¶: {time.time() - total_start:.1f} ç§’")


if __name__ == "__main__":
    run_scraper()
